# Workshop-ApacheSpark

## Team Members 

### 1.  Akhitha Tumula
<img src="GroupMembers/akhitha-picture.jpeg" alt="drawing" width="100"/>

### 2. SatishKumar Mandapalli
<img src="GroupMembers/satishkumar_Photo.jpg" alt="drawing" width="100"/>

### 3. Sai Chandu Gampa
<img src="GroupMembers/chandu.jpg" alt="drawing" width="100"/>

## Profile Links
- [Akhitha Tumula](https://github.com/thumula-akhitha)
- [Satishkumar Mandapalli](https://github.com/mandapallisatish64)
- [Sai Chandu Gampa](https://github.com/saichandugampa)

## Introduction
**Apache Spark** is an open source cluster computing platform for real-time processing. This is one of the most popular projects of the Apache Software Foundation. Spark has clearly emerged as the industry leader in the production of Big Data.Today, Spark is being embraced by top names including Amazon , eBay, and Yahoo! Several companies are running Spark on clusters of thousands of nodes.It has a thriving open-source community and is the most active Apache project at the moment. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.
- It was built on top of Hadoop MapReduce and it extends the MapReduce model to efficiently use more types of computations.
### Features of Apache Spark
1. **Polyglot**:Spark offers high-level APIs for Java, Scala, Python and R. The Spark code can be written in any of these four languages. It's providing a shell for Scala and Python. The Scala shell can be accessed from the installed directory via ```./bin / spark-shell``` and Python shell via ```./bin / pyspark```.
2. **Speed**:Spark is up to 100 times faster than Hadoop MapReduce for large-scale data processing. Spark is able to achieve this speed by means of controlled partitioning. It manages data using partitions that help to parallel distributed data processing with minimal network traffic.
3. **Multiple Formats**:Spark supports various data types, such as Parquet, JSON, Hive and Cassandra, in addition to the standard formats such as text files, CSV and RDBMS tables. The Data Source API provides a pluggable mechanism for accessing structured data through Spark SQL. Data sources can be more than just simple pipes that convert data and convert it to Spark.
4. **Lazy Evaluation**:Apache Spark is delaying its evaluation until it is absolutely necessary. This is one of the main factors that contributes to its size. For transformations, Spark adds them to the DAG (Directed Acyclic Graph) of the computation and only when the driver requests any data does the DAG actually get executed.
5. **Real Time Computation**:Spark's computation is real-time and has low latency due to its in-memory computation. Spark is designed for massive scalability and the Spark team has recorded users of the framework operating production clusters with thousands of nodes and assisted a variety of computational models.


We are going to discuss Apache spark with three different programming languages Scala, Java and Python.













## References
